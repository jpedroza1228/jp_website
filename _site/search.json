[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Looks At All My Stuff",
    "section": "",
    "text": "Prophet Model\n\n\nCreation of a Prophet model to forecast stock prices.\n\n\n\n\nVisualizations\n\n\nAnalysis\n\n\nForecast\n\n\nTidyModels\n\n\nModeltime\n\n\n\n\n\n\n\n\n\n\n\nJun 2, 2022\n\n\n\n\n\n\n  \n\n\n\n\nGraduate Student Satisfaction Exit Surveys\n\n\nVisualizations I created to examine how students from University of Oregon graduate programs perceived their departments/programs/advisors.\n\n\n\n\nVisualizations\n\n\nShiny\n\n\n\n\n\n\n\n\n\n\n\nJun 19, 2021\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday Coffee Ratings\n\n\nI decided to use TidyModels to create some models predicting Washed/Not Washed (Natural) Coffees.\n\n\n\n\nVisualizations\n\n\nAnalysis\n\n\nTidy Tuesday\n\n\n\n\n\n\n\n\n\n\n\nMay 13, 2021\n\n\n\n\n\n\n  \n\n\n\n\nTwitter Conference Presentation\n\n\nPost about my presentation I gave over Twitter for the American Public Health Association’s Physical Activity section.\n\n\n\n\nVisualizations\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\n\n\nApr 30, 2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Jonathan A. Pedroza (JP)",
    "section": "",
    "text": "My interests include examining environmental factors that contribute to inequities of access and engagement in health behaviors, creating visualizations to disseminate data to various audiences, and learning more about inferential statistics and predictive modeling techniques. I enjoy using R as my main programming language and am competent in Python. I also have an understanding of creating queries using SQL.\nAway from my computer I enjoy everything about coffee, hiking, cycling, and playing with my cat. If you have any questions, you can reach me on twitter or through email (cpppedroza@gmail.com)."
  },
  {
    "objectID": "posts/2021-04-30-grad-student-exit-surveys/index.html",
    "href": "posts/2021-04-30-grad-student-exit-surveys/index.html",
    "title": "Graduate Student Satisfaction Exit Surveys",
    "section": "",
    "text": "This data ended up becoming a real time commitment as there was no efficient way to collect data from the pdf files for each College at the UO. An example can be seen here. One great resource for collecting data from pdfs was to use the pdftools package, but if you look at the example link provided above the UO Graduate School decided to color code cells in the table, which threw off any function to extract all the values in an efficient manner. Anyway…\nThe data and other existing data files can be found here. When I have some more free time, I may decide to join the other datasets to the student experience data to examine some more interesting questions regarding this data. But for now, lets look at the student experience data.\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.0.5\n\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.1 --\n\n\nv ggplot2 3.3.6     v purrr   0.3.4\nv tibble  3.1.8     v dplyr   1.0.9\nv tidyr   1.2.0     v stringr 1.4.0\nv readr   2.1.2     v forcats 0.5.1\n\n\nWarning: package 'tidyr' was built under R version 4.0.5\n\n\nWarning: package 'readr' was built under R version 4.0.5\n\n\nWarning: package 'forcats' was built under R version 4.0.5\n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\ntheme_set(theme_minimal())\n\nexit <- read_csv(\"https://raw.githubusercontent.com/jpedroza1228/exitsurveys/main/data/student_experience.csv\") %>% \n  janitor::clean_names() \n\nRows: 103 Columns: 113\n\n\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr   (1): program\ndbl (112): year, number_respondents, fac_qual_ex, pro_qual_ex, money_sup_ex,...\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nexit$program <- str_replace_all(exit$program,\"_\", \" \")\nexit$program <- str_to_title(exit$program)\n\nThese exit surveys have several questions that are broken down into percentages about how many of the students agreed or disagreed with the statement. For instance, from the pdf, the first statement is Quality of the faculty in a student’s department. So we can look at that with this first plot. At the same time, we can also look at the difference between the two years of data. In order to look at all the variables at the same time that have the starting string of fac_qual, I’ll use pivot_longer to collect any variable that has that variable string about faculty quality. Since the first and second table on the pdf refer to excellent or good or excellent levels of student satisfaction about faculty quality, I decided to filter out the excellent student satisfaction and move on with only student satisfaction that is either good or excellent.\n\nexit %>% \n  pivot_longer(cols = tidyselect::vars_select(names(exit), starts_with(\"fac_qual\")),\n               names_to = \"fac_qual\", values_to = \"fac_values\") %>% \n  filter(fac_qual != \"fac_qual_ex\") %>%\n  ggplot(aes(fct_reorder(program, fac_values), fac_values)) +\n  geom_col(aes(fill = as.factor(year)), position = \"dodge2\") +\n  labs(title = \"Student Experiences by Academic Program\",\n       x = \"\",\n       y = \"Specific Student Experience\",\n       caption = \"Ex = Excellent\") +\n  coord_flip() +\n  facet_wrap(~fac_qual) +\n  theme(legend.position = \"bottom\",\n        legend.title = element_blank())\n\nWarning: Removed 20 rows containing missing values (geom_col).\n\n\n\n\n\nSo the first shot at making a visual for the two years looks a little cluttered because of using geom_col(). My first decision was to remove the columns and change those to points to make it a little less cluttered and clearer. I already enjoyed the way this looked better. I also decided to clean some things up by changing the names of the variables to better describe what the variables were assessing. I also decided to go back and change the programs to be title case and with spaces rather than underscores.\n\nexit %>% \n  pivot_longer(cols = tidyselect::vars_select(names(exit), starts_with(\"fac_qual\")),\n               names_to = \"fac_qual\", values_to = \"fac_values\") %>% \n  filter(fac_qual != \"fac_qual_ex\") %>%\n  mutate(fac_qual = recode(fac_qual, \"fac_qual_ex_good\" = \"Excellent/Good Faculty Quality\",\n                           \"fac_qual_fair_poor\" = \"Fair/Poor Faculty Quality\")) %>% \n  ggplot(aes(fct_reorder(program, fac_values), fac_values)) +\n  geom_point(aes(color = as.factor(year), shape = as.factor(year)), size = 2) +\n  labs(title = \"Faculty Quality by Academic Program\",\n       x = \"\",\n       y = \"Faculty Quality\",\n       caption = \"Data from University of Oregon's (UO)\\nstudent satisfaction surveys after graduation\") +\n  coord_flip() +\n  facet_wrap(~fac_qual) +\n  scale_color_manual(values = c(\"#d74122\",\"#669b3e\")) +\n  theme(legend.position = \"bottom\",\n        legend.title = element_blank())\n\nWarning: Removed 20 rows containing missing values (geom_point).\n\n\n\n\n\nJust in case anyone else is interested in this data, I also created a quick function to see how this visual looked like for other variables in the dataset. For instance, I’ll look at a couple of different variables.\n\nprogram_experience <- function(name){\n  experience <- {{name}}\n  \n  exit %>% \n    pivot_longer(cols = tidyselect::vars_select(names(exit), starts_with(glue::glue(\"{experience}\"))),\n               names_to = \"exp_names\", values_to = \"values\") %>% \n    filter(exp_names != glue::glue(\"{experience}_ex\") &\n             exp_names != glue::glue(\"{experience}_strong\")) %>% \n  ggplot(aes(fct_reorder(program, values), values)) +\n  geom_point(aes(color = as.factor(year), shape = as.factor(year)), size = 2) +\n  labs(title = \"Student Experiences by Academic Program\",\n       x = \"\",\n       y = \"\") +\n  coord_flip() +\n  facet_wrap(~exp_names) +\n  scale_color_manual(values = c(\"#d74122\",\"#669b3e\")) +\n  theme(legend.position = \"bottom\",\n        legend.title = element_blank())\n}\n\nBelow are all the variables from the dataset.\n\n\n  [1] \"year\"                          \"program\"                      \n  [3] \"number_respondents\"            \"fac_qual_ex\"                  \n  [5] \"pro_qual_ex\"                   \"money_sup_ex\"                 \n  [7] \"field_dev_pace_ex\"             \"advising_qual_ex\"             \n  [9] \"smart_community_ex\"            \"prof_dev_ex\"                  \n [11] \"equipment_ex\"                  \"grad_involve_ex\"              \n [13] \"research_opp_ex\"               \"grad_fair_assess_ex\"          \n [15] \"promote_inclu_ex\"              \"grant_train_ex\"               \n [17] \"teach_prep_ex\"                 \"grad_clear_assess_ex\"         \n [19] \"inter_sup_ex\"                  \"prof_ethic_train_ex\"          \n [21] \"fac_qual_ex_good\"              \"pro_qual_ex_good\"             \n [23] \"money_sup_ex_good\"             \"field_dev_pace_ex_good\"       \n [25] \"advising_qual_ex_good\"         \"smart_community_ex_good\"      \n [27] \"prof_dev_ex_good\"              \"equipment_ex_good\"            \n [29] \"grad_involve_ex_good\"          \"research_opp_ex_good\"         \n [31] \"grad_fair_assess_ex_good\"      \"promote_inclu_ex_good\"        \n [33] \"grant_train_ex_good\"           \"teach_prep_ex_good\"           \n [35] \"grad_clear_assess_ex_good\"     \"inter_sup_ex_good\"            \n [37] \"prof_ethic_train_ex_good\"      \"fac_qual_fair_poor\"           \n [39] \"pro_qual_fair_poor\"            \"money_sup_fair_poor\"          \n [41] \"field_dev_pace_fair_poor\"      \"advising_qual_fair_poor\"      \n [43] \"smart_community_fair_poor\"     \"prof_dev_fair_poor\"           \n [45] \"equipment_fair_poor\"           \"grad_involve_fair_poor\"       \n [47] \"research_opp_fair_poor\"        \"grad_fair_assess_fair_poor\"   \n [49] \"promote_inclu_fair_poor\"       \"grant_train_fair_poor\"        \n [51] \"teach_prep_fair_poor\"          \"grad_clear_assess_fair_poor\"  \n [53] \"inter_sup_fair_poor\"           \"prof_ethic_train_fair_poor\"   \n [55] \"encourage_agree\"               \"idea_resp_agree\"              \n [57] \"construct_feed_agree\"          \"time_feed_agree\"              \n [59] \"avail_agree\"                   \"career_sup_agree\"             \n [61] \"stu_equit_agree\"               \"ethic_emp_agree\"              \n [63] \"help_secure_fund_agree\"        \"help_prof_dev_agree\"          \n [65] \"publish_help_agree\"            \"encourage_intel_diff_agree\"   \n [67] \"comfort_talk_issue_agree\"      \"encourage_disagree\"           \n [69] \"idea_resp_disagree\"            \"construct_feed_disagree\"      \n [71] \"time_feed_disagree\"            \"avail_disagree\"               \n [73] \"career_sup_disagree\"           \"stu_equit_disagree\"           \n [75] \"ethic_emp_disagree\"            \"help_secure_fund_disagree\"    \n [77] \"help_prof_dev_disagree\"        \"publish_help_disagree\"        \n [79] \"encourage_intel_diff_disagree\" \"comfort_talk_issue_disagree\"  \n [81] \"collegial_strong\"              \"encouraging_strong\"           \n [83] \"supportive_strong\"             \"intel_open_strong\"            \n [85] \"inter_open_strong\"             \"inclu_stu_color_strong\"       \n [87] \"inclu_gender_strong\"           \"inclu_intern_stu_strong\"      \n [89] \"inclu_stu_disab_strong\"        \"inclu_first_gen_strong\"       \n [91] \"inclu_stu_sex_orient_strong\"   \"collegial_agree\"              \n [93] \"encouraging_agree\"             \"supportive_agree\"             \n [95] \"intel_open_agree\"              \"inter_open_agree\"             \n [97] \"inclu_stu_color_agree\"         \"inclu_gender_agree\"           \n [99] \"inclu_intern_stu_agree\"        \"inclu_stu_disab_agree\"        \n[101] \"inclu_first_gen_agree\"         \"inclu_stu_sex_orient_agree\"   \n[103] \"collegial_disagree\"            \"encouraging_disagree\"         \n[105] \"supportive_disagree\"           \"intel_open_disagree\"          \n[107] \"inter_open_disagree\"           \"inclu_stu_color_disagree\"     \n[109] \"inclu_gender_disagree\"         \"inclu_intern_stu_disagree\"    \n[111] \"inclu_stu_disab_disagree\"      \"inclu_first_gen_disagree\"     \n[113] \"inclu_stu_sex_orient_disagree\"\n\n\n\n# student equitable treatment\nprogram_experience(name = \"stu_equit\")\n\nWarning: Removed 20 rows containing missing values (geom_point).\n\n\n\n\n# inclusive of students of color\nprogram_experience(name = \"inclu_stu_color\")\n\nWarning: Removed 20 rows containing missing values (geom_point).\n\n\n\n\n# inclusive of gender\nprogram_experience(name = \"inclu_gender\")\n\nWarning: Removed 20 rows containing missing values (geom_point).\n\n\n\n\n# inclusive of international students\nprogram_experience(name = \"inclu_intern_stu\")\n\nWarning: Removed 20 rows containing missing values (geom_point).\n\n\n\n\n# inclusive of students with disabilities\nprogram_experience(name = \"inclu_stu_disab\")\n\nWarning: Removed 20 rows containing missing values (geom_point).\n\n\n\n\n# inclusive of first generation students\nprogram_experience(name = \"inclu_first_gen\")\n\nWarning: Removed 20 rows containing missing values (geom_point).\n\n\n\n\n# inclusive of students of all sexual orientations\nprogram_experience(name = \"inclu_stu_sex_orient\")\n\nWarning: Removed 20 rows containing missing values (geom_point).\n\n\n\n\n\nLastly, I decided to look into the difference between the variables I’m most interested in. First, I wanted to look at how graduate students perceive inclusiveness of students of color within their departments. Another variable I was interested in was inclusiveness of first-generation graduate students. Thanks to the plotly package I was able to include some interactive components to the visuals. Specifically zooming in to specific departments give a better idea of the difference between agreeing and disagreeing on these topics. With plotly, you can also click on an option in the legend to only see those values. I also removed the strongly agree option since the agree applied to students that strongly agreed or agreed with the statement.\n\nlibrary(plotly)\n\nWarning: package 'plotly' was built under R version 4.0.5\n\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\nstu_color <- exit %>% \n  pivot_longer(cols = tidyselect::vars_select(names(exit), starts_with(\"inclu_stu_color\")),\n               names_to = \"stu_color\", values_to = \"stu_color_values\") %>% \n  filter(stu_color != \"inclu_stu_color_strong\") %>% \n  mutate(stu_color = recode(stu_color, \"inclu_stu_color_agree\" = \"Agree with Inclusive Environment for Students of Color\",\n                           \"inclu_stu_color_disagree\" = \"Disagree with Inclusive Environment for Students of Color\")) %>% \n  ggplot(aes(fct_reorder(program, stu_color_values), stu_color_values)) +\n  geom_point(aes(color = as.factor(year), shape = as.factor(stu_color)), size = 2) +\n  labs(title = \"Faculty Quality by Academic Program\",\n       x = \"\",\n       y = \"Faculty Quality\",\n       caption = \"Data from University of Oregon's (UO)\\nstudent satisfaction surveys after graduation\") +\n  coord_flip() +\n  scale_color_manual(values = c(\"#d74122\",\"#669b3e\"))\n\nstu_plot <- ggplotly(stu_color)\n  # layout(legend = list(orientation = \"h\",\n                       # xanchor = \"center\",\n                       # x = 0,\n                       # y = -60)) \nstu_plot\n\n\n\n\nfirstgen <- exit %>% \n  pivot_longer(cols = tidyselect::vars_select(names(exit), starts_with(\"inclu_first_gen\")),\n               names_to = \"first_gen\", values_to = \"first_gen_values\") %>% \n  filter(first_gen != \"inclu_first_gen_strong\") %>% \n  mutate(first_gen = recode(first_gen, \"inclu_first_gen_agree\" = \"Agree with Inclusive Environment for First Gen\",\n                           \"inclu_first_gen_disagree\" = \"Disagree with Inclusive Environment for First Gen\")) %>% \n  ggplot(aes(fct_reorder(program, first_gen_values), first_gen_values)) +\n  geom_point(aes(color = as.factor(year), shape = as.factor(first_gen)), size = 2) +\n  labs(title = \"Faculty Quality by Academic Program\",\n       x = \"\",\n       y = \"Faculty Quality\",\n       caption = \"Data from University of Oregon's (UO)\\nstudent satisfaction surveys after graduation\") +\n  coord_flip() +\n  scale_color_manual(values = c(\"#d74122\",\"#669b3e\"))\n\nfirst_plot <- ggplotly(firstgen) \n  # layout(legend = list(orientation = \"h\",\n  #                      xanchor = \"center\",\n  #                      x = 0,\n  #                      y = -60)) \nfirst_plot"
  },
  {
    "objectID": "posts/2021-04-30-twitter-conference-presentation/index.html",
    "href": "posts/2021-04-30-twitter-conference-presentation/index.html",
    "title": "Twitter Conference Presentation",
    "section": "",
    "text": "Physical activity is important as its related to many physical and mental health conditions. It is also a health behavior that can be modified slightly easier than other health behaviors. While not as beneficial as extended periods of exercise, even walking for leisure can be beneficial for one’s health. I was predominately interested in California because I wanted to know how much variation there was between the counties. For instance, there are areas like San Francisco county and Los Angeles County, which may be seen as hubs for cultures of being physically active, but what about counties throughout Central California. I’m also interested in LTPA engagement because this health behavior has several social determinants of health that impact how much LTPA individuals can engage in. The social determinant that I’m most interested in is the role that access to recreational facilities and parks have on counties’ LTPA engagement. Since I was interested in looking at variation between counties while also examining the longitudinal association between access and LTPA, I decided to create a two-level multilevel model with time (level 1) nested within counties (level 2).\n\nPackages ued\n\n\nWarning: package 'tidyverse' was built under R version 4.0.5\n\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.1 --\n\n\nv ggplot2 3.3.6     v purrr   0.3.4\nv tibble  3.1.8     v dplyr   1.0.9\nv tidyr   1.2.0     v stringr 1.4.0\nv readr   2.1.2     v forcats 0.5.1\n\n\nWarning: package 'tidyr' was built under R version 4.0.5\n\n\nWarning: package 'readr' was built under R version 4.0.5\n\n\nWarning: package 'forcats' was built under R version 4.0.5\n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\n\nWarning: package 'inspectdf' was built under R version 4.0.5\n\n\n\nAttaching package: 'psych'\n\n\nThe following objects are masked from 'package:ggplot2':\n\n    %+%, alpha\n\n\nLoading required package: Matrix\n\n\nWarning: package 'Matrix' was built under R version 4.0.5\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\n\nAttaching package: 'lmerTest'\n\n\nThe following object is masked from 'package:lme4':\n\n    lmer\n\n\nThe following object is masked from 'package:stats':\n\n    step\n\n\nGoogle's Terms of Service: https://cloud.google.com/maps-platform/terms/.\n\n\nPlease cite ggmap if you use it! See citation(\"ggmap\") for details.\n\n\nWarning: package 'maps' was built under R version 4.0.5\n\n\n\nAttaching package: 'maps'\n\n\nThe following object is masked from 'package:purrr':\n\n    map\n\n\nWarning: package 'RColorBrewer' was built under R version 4.0.5\n\n\nWarning: package 'ggrepel' was built under R version 4.0.5\n\n\nWarning: package 'gganimate' was built under R version 4.0.4\n\n\nWarning: package 'transformr' was built under R version 4.0.5\n\n\n\n\n\n\n\nFunctions\nBefore beginning I created a simple function to get the intraclass correlation coefficient (ICC). I also included a function to get data from the 5 years of County Health Rankings data. The function to get the ICC from random-intercept models is the between county variation divided by the total variation between counties and within counties. This gives you information about how much of the variation in the model can be attributed to differences between counties regarding your outcome. Random-intercept models were tested because I was only interested in knowing the variation in counties’ LTPA engagement. There were also not enough points for each county to test individual slopes for each county (random-slopes model).\n\n\n\nI also made some slight changes to my data. The first was to get rid of the estimates for each state and only focus on estimates from the county. I also wanted to treat year as a continuous variable in my models but wanted to keep the year variable as a factor too. Then after filtering to only examine California counties I used the str_replace_all function from the stringr package to get rid of the county name after each observation. This was to make it easier to join with map data from the maps package. Lastly, I made the counties title case to also make joining the observations easier.\n\n\n\n\n\nModels\nNow when running the first model, I was first interested in examining if there was an increase in LTPA engagement in all California counties from 2016 to 2020. From the finding below, it shows that in California, there was a decrease in LTPA over that time. It’s also important to note that lmerTest and lme4 both have a lmer function. By namespacing them with two colons, you can see that the summary information is slightly different.\n\npreliminary_ltpa_long <- lmerTest::lmer(ltpa_percent ~ year_num + (1 | county_fips_code), data = ca,\n                              REML = FALSE)\nsummary(preliminary_ltpa_long)\n\nLinear mixed model fit by maximum likelihood . t-tests use Satterthwaite's\n  method [lmerModLmerTest]\nFormula: ltpa_percent ~ year_num + (1 | county_fips_code)\n   Data: ca\n\n     AIC      BIC   logLik deviance df.resid \n  1427.9   1442.5   -709.9   1419.9      286 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.6854 -0.3709  0.0361  0.5377  1.7084 \n\nRandom effects:\n Groups           Name        Variance Std.Dev.\n county_fips_code (Intercept) 7.187    2.681   \n Residual                     5.174    2.275   \nNumber of obs: 290, groups:  county_fips_code, 58\n\nFixed effects:\n              Estimate Std. Error         df t value            Pr(>|t|)    \n(Intercept) 1923.40172  190.60225  231.84941  10.091 <0.0000000000000002 ***\nyear_num      -0.91276    0.09445  231.84760  -9.664 <0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n         (Intr)\nyear_num -1.000\n\nprelim_ltpa_lmer <- lme4::lmer(ltpa_percent ~ year_num +(1 | county_fips_code), data = ca,\n                              REML = FALSE)\nsummary(prelim_ltpa_lmer)\n\nLinear mixed model fit by maximum likelihood  ['lmerMod']\nFormula: ltpa_percent ~ year_num + (1 | county_fips_code)\n   Data: ca\n\n     AIC      BIC   logLik deviance df.resid \n  1427.9   1442.5   -709.9   1419.9      286 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.6854 -0.3709  0.0361  0.5377  1.7084 \n\nRandom effects:\n Groups           Name        Variance Std.Dev.\n county_fips_code (Intercept) 7.187    2.681   \n Residual                     5.174    2.275   \nNumber of obs: 290, groups:  county_fips_code, 58\n\nFixed effects:\n              Estimate Std. Error t value\n(Intercept) 1923.40172  190.60225  10.091\nyear_num      -0.91276    0.09445  -9.664\n\nCorrelation of Fixed Effects:\n         (Intr)\nyear_num -1.000\n\nltpa_null_icc <- as_tibble(VarCorr(preliminary_ltpa_long))\nltpa_null_icc\n\n# A tibble: 2 x 5\n  grp              var1        var2   vcov sdcor\n  <chr>            <chr>       <chr> <dbl> <dbl>\n1 county_fips_code (Intercept) <NA>   7.19  2.68\n2 Residual         <NA>        <NA>   5.17  2.27\n\ncounty_icc_2level(ltpa_null_icc)\n\n[1] 0.5814093\n\n\nAlong with the fixed effects, we also got our random effects for both differences found between counties for LTPA engagement and differences within counties for LTPA engagement. This shows that there was a good amount of variation between counties (σ2 = 7.19) but also a large amount of variation within each county in California (σ2 = 5.17). Using the function to calculate the ICC, it found that county differences attributed to 58% of the variation in LTPA engagement. Something that should be considered is the potential for heteroscedastic residual variance at level 1. There is also the issue that the residuals could suggest spatial autocorrelation or clustering within these counties. Maybe I’ll create something on these soon. But for the time being, lets move on to what was found for the twitter conference.\n\nltpa_long_access <- lmer(ltpa_percent ~ year_num + violent_crime + obesity_percent + median_household_income + rural_percent +\n                           access_pa_percent + (1 | county_fips_code), data = ca,\n                         REML = FALSE)\n\nWarning: Some predictor variables are on very different scales: consider\nrescaling\n\nWarning: Some predictor variables are on very different scales: consider\nrescaling\n\nanova(preliminary_ltpa_long, ltpa_long_access)\n\nData: ca\nModels:\npreliminary_ltpa_long: ltpa_percent ~ year_num + (1 | county_fips_code)\nltpa_long_access: ltpa_percent ~ year_num + violent_crime + obesity_percent + median_household_income + rural_percent + access_pa_percent + (1 | county_fips_code)\n                      npar    AIC    BIC  logLik deviance  Chisq Df\npreliminary_ltpa_long    4 1427.9 1442.5 -709.93   1419.9          \nltpa_long_access         9 1237.4 1270.4 -609.69   1219.4 200.47  5\n                                 Pr(>Chisq)    \npreliminary_ltpa_long                          \nltpa_long_access      < 0.00000000000000022 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nother_var <- lmer(ltpa_percent ~ year_num + violent_crime + obesity_percent + median_household_income + rural_percent + (1 | county_fips_code), data = ca,\n                         REML = FALSE)\n\nWarning: Some predictor variables are on very different scales: consider\nrescaling\n\nWarning: Some predictor variables are on very different scales: consider\nrescaling\n\nanova(other_var, ltpa_long_access)\n\nData: ca\nModels:\nother_var: ltpa_percent ~ year_num + violent_crime + obesity_percent + median_household_income + rural_percent + (1 | county_fips_code)\nltpa_long_access: ltpa_percent ~ year_num + violent_crime + obesity_percent + median_household_income + rural_percent + access_pa_percent + (1 | county_fips_code)\n                 npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)  \nother_var           8 1240.1 1269.4 -612.04   1224.1                       \nltpa_long_access    9 1237.4 1270.4 -609.69   1219.4 4.6883  1    0.03037 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWith the inclusion of several predictors for fixed effects, a likelihood ratio test was conducted to see if the inclusion of these fixed effects revealed a significantly better fitting model. The inclusion of these predictors revealed a better fitting model. It would probably be better to see if the inclusion of one variable of interest, such as access, resulted in a better fitting model than a model with the other social determinants of health. As can be see here, the likelihood ratio test of including only access still resulted in a signifcantly better fitting model.\n\nsummary(ltpa_long_access)\n\nLinear mixed model fit by maximum likelihood . t-tests use Satterthwaite's\n  method [lmerModLmerTest]\nFormula: \nltpa_percent ~ year_num + violent_crime + obesity_percent + median_household_income +  \n    rural_percent + access_pa_percent + (1 | county_fips_code)\n   Data: ca\n\n     AIC      BIC   logLik deviance df.resid \n  1237.4   1270.4   -609.7   1219.4      281 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.8388 -0.5546  0.0010  0.6179  2.4921 \n\nRandom effects:\n Groups           Name        Variance Std.Dev.\n county_fips_code (Intercept) 1.286    1.134   \n Residual                     3.139    1.772   \nNumber of obs: 290, groups:  county_fips_code, 58\n\nFixed effects:\n                              Estimate     Std. Error             df t value\n(Intercept)             1138.319788532  193.185926311  289.534084001   5.892\nyear_num                  -0.517087679    0.096244053  289.365967843  -5.373\nviolent_crime             -0.001434185    0.001190066   82.732732827  -1.205\nobesity_percent           -0.602338792    0.043760246  262.840285406 -13.765\nmedian_household_income    0.000004822    0.000014701  102.255260100   0.328\nrural_percent             -0.012178273    0.007671490   63.272282539  -1.587\naccess_pa_percent          0.027194696    0.012124625  153.692013407   2.243\n                                    Pr(>|t|)    \n(Intercept)                     0.0000000106 ***\nyear_num                        0.0000001597 ***\nviolent_crime                         0.2316    \nobesity_percent         < 0.0000000000000002 ***\nmedian_household_income               0.7436    \nrural_percent                         0.1174    \naccess_pa_percent                     0.0263 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) yer_nm vlnt_c obsty_ mdn_h_ rrl_pr\nyear_num    -1.000                                   \nviolent_crm  0.116 -0.118                            \nobsty_prcnt  0.489 -0.494 -0.100                     \nmdn_hshld_n  0.586 -0.589  0.203  0.452              \nrural_prcnt  0.260 -0.264  0.135  0.203  0.447       \naccss_p_prc -0.147  0.142 -0.018  0.054 -0.341  0.158\nfit warnings:\nSome predictor variables are on very different scales: consider rescaling\n\n\nThe model summary suggests that the fixed effect of access on LTPA engagement was significantly associated. The thing that stands out the most here is that the inclusion of the predictors resulted in more variation within counties than between counties. So lets look into that more closely.\n\nltpa_access_icc <- as_tibble(VarCorr(ltpa_long_access))\nltpa_access_icc\n\n# A tibble: 2 x 5\n  grp              var1        var2   vcov sdcor\n  <chr>            <chr>       <chr> <dbl> <dbl>\n1 county_fips_code (Intercept) <NA>   1.29  1.13\n2 Residual         <NA>        <NA>   3.14  1.77\n\ncounty_icc_2level(ltpa_access_icc)\n\n[1] 0.2906253\n\n\nThe ICC suggests that 29% of the variation explained is from differences between counties. It is also beneficial to look at all of this through visuals.\n\n\nVisuals Prep\nBelow we’ll start by using the maps package to get county-level data of the contiguous United States. The steps below were to make sure this data frame joined with the county health rankings data we had created previously.\n\n\nVisualizing model\nOne way to visualize the variation between counties in our final model (ltpa_long_access) is to use a caterpillar plot. This allows you to view variation in the residuals of each county for your outcome. From the visual, you can see the differences between Humboldt County and Tehama County.\n\nmain_effects_var <- ranef(ltpa_long_access, condVar = TRUE)\n\nmain_effects_var <- as.data.frame(main_effects_var)\n\nmain_effects_var <- main_effects_var %>% \n  mutate(main_effects_term = term,\n         county_fips_code = grp,\n         main_effects_diff = condval,\n         main_effects_se = condsd,\n         county_fips_code = as.numeric(county_fips_code))\n\nmain_effects_var$no_name_county <- unique(ca$no_name_county)\n\nmain_effects_var %>% \n  ggplot(aes(fct_reorder(no_name_county, main_effects_diff), main_effects_diff)) +\n  geom_errorbar(aes(ymin = main_effects_diff + qnorm(0.025)*main_effects_se,\n                  ymax = main_effects_diff + qnorm(0.975)*main_effects_se)) +\n  geom_point(aes(color = no_name_county)) +\n  coord_flip() +\n  labs(x = ' ',\n     y = 'Differences in Leisure-time Physical Activity',\n     title = 'Variation in Leisure-time Physical Activity\\nAcross California Counties') +\n  theme(legend.position = 'none')\n\n\n\n\nThis plot shows the fixed effect of access and LTPA across the various years.\n\nca %>% \n  mutate(year = as.factor(year)) %>% \n  ggplot(aes(access_pa_percent, ltpa_percent)) +\n  geom_point(aes(color = year)) +\n  geom_smooth(color = 'dodgerblue',\n            method = 'lm', se = FALSE, size = 1) +\n  theme(legend.title = element_blank()) +\n  labs(x = 'Access to Physical Activity Opportunities',\n       y = 'Leisure-time Physical Activity',\n       title = 'The Statewide Association of Access\\nand Physical Activity')\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nFinally, a gif of the change of LTPA from 2016 to 2020.\n\nlibrary(gganimate)\n\nca_animate <- ca_visual %>%\n  ggplot(aes(frame = year,\n             cumulative = TRUE)) +\n  geom_polygon(aes(x = long, y = lat, \n                   group = group, \n                   fill = ltpa_percent),\n               color = 'black') +\n  scale_fill_gradientn(colors = brewer.pal(n = 5, name = 'RdYlGn')) + \n  theme_classic() +\n  transition_time(year) +\n  labs(x = 'Longitude',\n       y = 'Latitude',\n       title = 'Leisure-time Physical Activity\\nChange Over Time',\n       subtitle = 'Year: {frame_time}') +\n  theme(legend.title = element_blank(),\n        legend.text = element_text(size = 12),\n        axis.text.x = element_text(size = 10),\n        axis.text.y = element_text(size = 10),\n        plot.title = element_text(size = 20),\n        plot.subtitle = element_text(size = 18))\n\nlibrary(gifski)\n\nWarning: package 'gifski' was built under R version 4.0.5\n\nanimate(ca_animate, renderer = gifski_renderer())"
  },
  {
    "objectID": "posts/2021-05-13-tidy-tuesday-coffee-ratings/index.html",
    "href": "posts/2021-05-13-tidy-tuesday-coffee-ratings/index.html",
    "title": "Tidy Tuesday Coffee Ratings",
    "section": "",
    "text": "With coffee being a hobby of mine, I was scrolling through past Tidy Tuesdays and found one on coffee ratings. Originally I thought looking at predictions of total cup points, but I assumed with all the coffee tasting characteristics that it wouldn’t really tell me anything. Instead, I decided to look into the processing method, as there are different taste characteristics between washed and other processing methods.\n\ncoffee <- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-07-07/coffee_ratings.csv') %>% \n  mutate(species = as.factor(species),\n         process = recode(processing_method, \"Washed / Wet\" = \"washed\",\n                          \"Semi-washed / Semi-pulped\" = \"not_washed\",\n                          \"Pulped natural / honey\" = \"not_washed\",\n                          \"Other\" = \"not_washed\",\n                          \"Natural / Dry\" = \"not_washed\",\n                          \"NA\" = NA_character_),\n         process = as.factor(process),\n         process = relevel(process, ref = \"washed\"),\n         country_of_origin = as.factor(country_of_origin)) %>% \n  drop_na(process) %>% \n  filter(country_of_origin != \"Cote d?Ivoire\")\n\nRows: 1339 Columns: 43\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (24): species, owner, country_of_origin, farm_name, lot_number, mill, ic...\ndbl (19): total_cup_points, number_of_bags, aroma, flavor, aftertaste, acidi...\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAfter looking at the distributions of procssing methods, I also decided to make the processing method binary with washed and not washed. This worked out better for the prediction models. There are also some descriptives of each variable.\n\ncoffee %>% \n  ggplot(aes(processing_method)) +\n  geom_bar(color = \"white\", fill = \"dodgerblue\") +\n  coord_flip()\n\n\n\ncoffee %>% \n  ggplot(aes(process)) +\n  geom_bar(color = \"white\", fill = \"dodgerblue\") +\n  coord_flip()\n\n\n\npsych::describe(coffee, na.rm = TRUE)[c(\"n\", \"mean\", \"sd\", \"min\", \"max\", \"skew\", \"kurtosis\")]\n\n                          n    mean      sd   min       max  skew kurtosis\ntotal_cup_points       1168   82.06    2.71 59.83     90.58 -1.95     9.26\nspecies*               1168    1.01    0.09  1.00      2.00 10.65   111.61\nowner*                 1161  133.43   76.78  1.00    287.00  0.10    -1.00\ncountry_of_origin*     1168   14.79   10.08  1.00     36.00  0.31    -1.12\nfarm_name*              883  271.04  154.16  1.00    524.00  0.00    -1.27\nlot_number*             239   93.18   59.86  1.00    202.00  0.18    -1.27\nmill*                   931  196.60  122.71  1.00    419.00  0.24    -1.28\nico_number*            1057  360.89  242.05  1.00    753.00  0.08    -1.27\ncompany*               1077  141.73   76.77  1.00    266.00 -0.10    -1.23\naltitude*              1014  154.54   98.40  1.00    351.00  0.32    -1.04\nregion*                1137  174.91   87.74  1.00    325.00 -0.11    -1.09\nproducer*               995  314.12  175.27  1.00    624.00 -0.04    -1.12\nnumber_of_bags         1168  153.80  130.08  1.00   1062.00  0.37     0.50\nbag_weight*            1168   23.50   16.67  1.00     45.00 -0.23    -1.71\nin_country_partner*    1168    9.68    7.04  1.00     25.00  0.41    -1.31\nharvest_year*          1161    5.82    2.95  1.00     14.00  0.76    -0.46\ngrading_date*          1168  242.39  144.52  1.00    495.00  0.11    -1.21\nowner_1*               1161  135.29   77.68  1.00    290.00  0.09    -1.00\nvariety*               1089   12.61    9.77  1.00     29.00  0.63    -1.24\nprocessing_method*     1168    3.98    1.67  1.00      5.00 -1.13    -0.62\naroma                  1168    7.56    0.31  5.08      8.75 -0.55     4.46\nflavor                 1168    7.51    0.34  6.08      8.83 -0.34     1.73\naftertaste             1168    7.39    0.34  6.17      8.67 -0.45     1.36\nacidity                1168    7.53    0.31  5.25      8.75 -0.30     3.31\nbody                   1168    7.52    0.28  6.33      8.50 -0.10     0.89\nbalance                1168    7.51    0.34  6.08      8.58 -0.10     1.17\nuniformity             1168    9.84    0.50  6.00     10.00 -4.21    20.83\nclean_cup              1168    9.84    0.75  0.00     10.00 -6.98    62.29\nsweetness              1168    9.89    0.52  1.33     10.00 -7.53    80.78\ncupper_points          1168    7.48    0.40  5.17      8.75 -0.64     2.79\nmoisture               1168    0.09    0.05  0.00      0.17 -1.41     0.35\ncategory_one_defects   1168    0.51    2.70  0.00     63.00 14.43   279.42\nquakers                1167    0.17    0.82  0.00     11.00  6.87    57.30\ncolor*                 1070    2.80    0.64  1.00      4.00 -1.51     2.57\ncategory_two_defects   1168    3.79    5.54  0.00     55.00  3.54    18.53\nexpiration*            1168  241.61  144.11  1.00    494.00  0.12    -1.21\ncertification_body*    1168    9.38    6.65  1.00     24.00  0.37    -1.31\ncertification_address* 1168   16.58    7.33  1.00     29.00 -0.19    -1.06\ncertification_contact* 1168    9.39    7.26  1.00     26.00  0.36    -0.96\nunit_of_measurement*   1168    1.87    0.34  1.00      2.00 -2.21     2.87\naltitude_low_meters    1012 1796.86 9073.21  1.00 190164.00 19.36   384.12\naltitude_high_meters   1012 1834.27 9071.86  1.00 190164.00 19.36   384.03\naltitude_mean_meters   1012 1815.56 9072.31  1.00 190164.00 19.36   384.12\nprocess*               1168    1.30    0.46  1.00      2.00  0.86    -1.27\n\n\nNow, its time to split the data into training and testing data. I also included the function of strata to stratify sampling based on process.\n\nset.seed(05132021)\n\ncoffee_split <- initial_split(coffee, strata = \"process\")\n\ncoffee_train <- training(coffee_split)\ncoffee_test <- testing(coffee_split)\n\nI also did some cross validation for the training dataset and used the metrics I was most interested in.\n\nset.seed(05132021)\n\ncoffee_fold <- vfold_cv(coffee_train, strata = \"process\", v = 10)\n\nmetric_measure <- metric_set(accuracy, mn_log_loss, roc_auc)\n\nFrom the beginning I was interested in the tasting characteristics and how they would predict whether the green coffee was washed or not washed. I also included the total cup points because I wanted to see the importance of that predictor on the processing method. The only feature engineering I did was to remove any zero variance in the predictors of the model.\n\nset.seed(05132021)\n\nchar_recipe <- recipe(process ~ aroma + flavor + aftertaste +\n                        acidity + body + balance + uniformity + clean_cup +\n                        sweetness + total_cup_points,\n                      data = coffee_train) %>% \n  step_zv(all_predictors(), -all_outcomes())\n\nchar_recipe %>% \n  prep() %>% \n  bake(new_data = NULL) %>%\n  head()\n\n# A tibble: 6 x 11\n  aroma flavor aftertaste acidity  body balance unifor~1 clean~2 sweet~3 total~4\n  <dbl>  <dbl>      <dbl>   <dbl> <dbl>   <dbl>    <dbl>   <dbl>   <dbl>   <dbl>\n1  8.17   8.58       8.42    8.42  8.5     8.25       10      10      10    89  \n2  8.08   8.58       8.5     8.5   7.67    8.42       10      10      10    88.2\n3  8.17   8.17       8       8.17  8.08    8.33       10      10      10    87.2\n4  8.42   8.17       7.92    8.17  8.33    8          10      10      10    87.1\n5  8.5    8.5        8       8     8       8          10      10      10    86.9\n6  8      8          8       8.25  8       8.17       10      10      10    86.6\n# ... with 1 more variable: process <fct>, and abbreviated variable names\n#   1: uniformity, 2: clean_cup, 3: sweetness, 4: total_cup_points\n# i Use `colnames()` to see all variable names\n\n\n\n\nThe first model I wanted to test with the current recipe was logistic regression. The accuracy and roc auc were alright for a starting model.\n\n\nWarning: package 'glmnet' was built under R version 4.0.5\n\n\nWarning: package 'Matrix' was built under R version 4.0.5\n\n\n\ncollect_metrics(lr_fit)\n\n# A tibble: 3 x 6\n  .metric     .estimator  mean     n std_err .config             \n  <chr>       <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy    binary     0.698    10 0.00342 Preprocessor1_Model1\n2 mn_log_loss binary     0.589    10 0.00775 Preprocessor1_Model1\n3 roc_auc     binary     0.648    10 0.0188  Preprocessor1_Model1\n\n\n\n\n\nNow for the first penalized regression. The lasso regression did not improve in either metric. Let’s try the next penalized regression.\n\n\n\n\ncollect_metrics(lasso_fit)\n\n# A tibble: 3 x 6\n  .metric     .estimator  mean     n std_err .config             \n  <chr>       <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy    binary     0.702    10 0.00397 Preprocessor1_Model1\n2 mn_log_loss binary     0.592    10 0.00850 Preprocessor1_Model1\n3 roc_auc     binary     0.645    10 0.0191  Preprocessor1_Model1\n\n\n\n\n\nThe ridge regression was shown to not be a good fitting model. So I tested an additional penalized regression while tuning hyper-parameters.\n\n\n\n\ncollect_metrics(ridge_fit)\n\n\n\n\nThe elastic net regression had slightly better accuracy than the non-penalized logistic regression but the ROC AUC was exactly the same. While the elastic net regression did not take long computationally due to the small amount of data, this model would not be chosen over the logistic regression.\n\n\n\n\ncollect_metrics(elastic_fit)\n\n# A tibble: 300 x 8\n         penalty mixture .metric     .estimator  mean     n std_err .config     \n           <dbl>   <dbl> <chr>       <chr>      <dbl> <int>   <dbl> <chr>       \n 1 0.0000000001        0 accuracy    binary     0.698    10 0.00342 Preprocesso~\n 2 0.0000000001        0 mn_log_loss binary     0.589    10 0.00775 Preprocesso~\n 3 0.0000000001        0 roc_auc     binary     0.648    10 0.0188  Preprocesso~\n 4 0.00000000129       0 accuracy    binary     0.698    10 0.00342 Preprocesso~\n 5 0.00000000129       0 mn_log_loss binary     0.589    10 0.00775 Preprocesso~\n 6 0.00000000129       0 roc_auc     binary     0.648    10 0.0188  Preprocesso~\n 7 0.0000000167        0 accuracy    binary     0.698    10 0.00342 Preprocesso~\n 8 0.0000000167        0 mn_log_loss binary     0.589    10 0.00775 Preprocesso~\n 9 0.0000000167        0 roc_auc     binary     0.648    10 0.0188  Preprocesso~\n10 0.000000215         0 accuracy    binary     0.698    10 0.00342 Preprocesso~\n# ... with 290 more rows\n# i Use `print(n = ...)` to see more rows\n\nshow_best(elastic_fit, metric = \"accuracy\", n = 5)\n\n# A tibble: 5 x 8\n        penalty mixture .metric  .estimator  mean     n std_err .config         \n          <dbl>   <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>           \n1 0.0774          0.111 accuracy binary     0.703    10 0.00434 Preprocessor1_M~\n2 0.0000000001    0.333 accuracy binary     0.703    10 0.00422 Preprocessor1_M~\n3 0.00000000129   0.333 accuracy binary     0.703    10 0.00422 Preprocessor1_M~\n4 0.0000000167    0.333 accuracy binary     0.703    10 0.00422 Preprocessor1_M~\n5 0.000000215     0.333 accuracy binary     0.703    10 0.00422 Preprocessor1_M~\n\nshow_best(elastic_fit, metric = \"roc_auc\", n = 5)\n\n# A tibble: 5 x 8\n        penalty mixture .metric .estimator  mean     n std_err .config          \n          <dbl>   <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>            \n1 0.0000000001        0 roc_auc binary     0.648    10  0.0188 Preprocessor1_Mo~\n2 0.00000000129       0 roc_auc binary     0.648    10  0.0188 Preprocessor1_Mo~\n3 0.0000000167        0 roc_auc binary     0.648    10  0.0188 Preprocessor1_Mo~\n4 0.000000215         0 roc_auc binary     0.648    10  0.0188 Preprocessor1_Mo~\n5 0.00000278          0 roc_auc binary     0.648    10  0.0188 Preprocessor1_Mo~\n\nselect_best(elastic_fit, metric = \"accuracy\")\n\n# A tibble: 1 x 3\n  penalty mixture .config               \n    <dbl>   <dbl> <chr>                 \n1  0.0774   0.111 Preprocessor1_Model019\n\nselect_best(elastic_fit, metric = \"roc_auc\")\n\n# A tibble: 1 x 3\n       penalty mixture .config               \n         <dbl>   <dbl> <chr>                 \n1 0.0000000001       0 Preprocessor1_Model001\n\n\n\n\n\nEven though the elastic net regression was only slightly better, I decided to update the workflow using that model. This time I decided to update the recipe by including additional predictors like if there were any defects in the green coffee beans, the species of the coffee (e.g., Robusta and Arabica), and the country of origin. I also included additional steps in my recipe by transforming the category predictors and working with the factor predictors, like species, and country of origin. The inclusion of additional steps and the predictors created a better fitting model with the elastic net regression.\n\nset.seed(05132021)\n\nbal_rec <- recipe(process ~ aroma + flavor + aftertaste +\n                        acidity + body + balance + uniformity + clean_cup +\n                        sweetness + total_cup_points + category_one_defects + category_two_defects + species +\n                        country_of_origin,\n                      data = coffee_train) %>% \n  step_BoxCox(category_two_defects, category_one_defects) %>% \n  step_novel(species, country_of_origin) %>% \n  step_other(species, country_of_origin, threshold = .01) %>%\n  step_unknown(species, country_of_origin) %>% \n  step_dummy(species, country_of_origin) %>% \n  step_zv(all_predictors(), -all_outcomes())\n\n\n\n! Fold01: preprocessor 1/1: Non-positive values in selected variable., No Box-Cox ...\n\n\n! Fold02: preprocessor 1/1: Non-positive values in selected variable., No Box-Cox ...\n\n\n! Fold03: preprocessor 1/1: Non-positive values in selected variable., No Box-Cox ...\n\n\n! Fold04: preprocessor 1/1: Non-positive values in selected variable., No Box-Cox ...\n\n\n! Fold05: preprocessor 1/1: Non-positive values in selected variable., No Box-Cox ...\n\n\n! Fold06: preprocessor 1/1: Non-positive values in selected variable., No Box-Cox ...\n\n\n! Fold07: preprocessor 1/1: Non-positive values in selected variable., No Box-Cox ...\n\n\n! Fold08: preprocessor 1/1: Non-positive values in selected variable., No Box-Cox ...\n\n\n! Fold09: preprocessor 1/1: Non-positive values in selected variable., No Box-Cox ...\n\n\n! Fold10: preprocessor 1/1: Non-positive values in selected variable., No Box-Cox ...\n\n\n\ncollect_metrics(elastic_bal_fit) \n\n# A tibble: 300 x 8\n         penalty mixture .metric     .estimator  mean     n std_err .config     \n           <dbl>   <dbl> <chr>       <chr>      <dbl> <int>   <dbl> <chr>       \n 1 0.0000000001        0 accuracy    binary     0.839    10 0.00923 Preprocesso~\n 2 0.0000000001        0 mn_log_loss binary     0.431    10 0.0153  Preprocesso~\n 3 0.0000000001        0 roc_auc     binary     0.840    10 0.0163  Preprocesso~\n 4 0.00000000129       0 accuracy    binary     0.839    10 0.00923 Preprocesso~\n 5 0.00000000129       0 mn_log_loss binary     0.431    10 0.0153  Preprocesso~\n 6 0.00000000129       0 roc_auc     binary     0.840    10 0.0163  Preprocesso~\n 7 0.0000000167        0 accuracy    binary     0.839    10 0.00923 Preprocesso~\n 8 0.0000000167        0 mn_log_loss binary     0.431    10 0.0153  Preprocesso~\n 9 0.0000000167        0 roc_auc     binary     0.840    10 0.0163  Preprocesso~\n10 0.000000215         0 accuracy    binary     0.839    10 0.00923 Preprocesso~\n# ... with 290 more rows\n# i Use `print(n = ...)` to see more rows\n\nshow_best(elastic_bal_fit, metric = \"accuracy\", n = 5)\n\n# A tibble: 5 x 8\n   penalty mixture .metric  .estimator  mean     n std_err .config              \n     <dbl>   <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n1 0.000464   0.111 accuracy binary     0.840    10 0.0101  Preprocessor1_Model0~\n2 0.000464   0.222 accuracy binary     0.840    10 0.0101  Preprocessor1_Model0~\n3 0.00599    0.111 accuracy binary     0.840    10 0.00933 Preprocessor1_Model0~\n4 0.00599    0.222 accuracy binary     0.840    10 0.00933 Preprocessor1_Model0~\n5 0.00599    0.333 accuracy binary     0.840    10 0.00933 Preprocessor1_Model0~\n\nshow_best(elastic_bal_fit, metric = \"mn_log_loss\", n = 5)\n\n# A tibble: 5 x 8\n   penalty mixture .metric     .estimator  mean     n std_err .config           \n     <dbl>   <dbl> <chr>       <chr>      <dbl> <int>   <dbl> <chr>             \n1 0.000464   1     mn_log_loss binary     0.420    10  0.0179 Preprocessor1_Mod~\n2 0.000464   0.889 mn_log_loss binary     0.420    10  0.0178 Preprocessor1_Mod~\n3 0.000464   0.778 mn_log_loss binary     0.420    10  0.0178 Preprocessor1_Mod~\n4 0.000464   0.667 mn_log_loss binary     0.420    10  0.0178 Preprocessor1_Mod~\n5 0.000464   0.556 mn_log_loss binary     0.420    10  0.0177 Preprocessor1_Mod~\n\nshow_best(elastic_bal_fit, metric = \"roc_auc\", n = 5)\n\n# A tibble: 5 x 8\n   penalty mixture .metric .estimator  mean     n std_err .config               \n     <dbl>   <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                 \n1 0.00599    0.778 roc_auc binary     0.843    10  0.0150 Preprocessor1_Model078\n2 0.000464   0.667 roc_auc binary     0.843    10  0.0141 Preprocessor1_Model067\n3 0.00599    0.889 roc_auc binary     0.843    10  0.0148 Preprocessor1_Model088\n4 0.000464   0.444 roc_auc binary     0.843    10  0.0141 Preprocessor1_Model047\n5 0.000464   0.556 roc_auc binary     0.843    10  0.0141 Preprocessor1_Model057\n\nselect_best(elastic_bal_fit, metric = \"accuracy\")\n\n# A tibble: 1 x 3\n   penalty mixture .config               \n     <dbl>   <dbl> <chr>                 \n1 0.000464   0.111 Preprocessor1_Model017\n\nselect_best(elastic_bal_fit, metric = \"mn_log_loss\")\n\n# A tibble: 1 x 3\n   penalty mixture .config               \n     <dbl>   <dbl> <chr>                 \n1 0.000464       1 Preprocessor1_Model097\n\nselect_best(elastic_bal_fit, metric = \"roc_auc\")\n\n# A tibble: 1 x 3\n  penalty mixture .config               \n    <dbl>   <dbl> <chr>                 \n1 0.00599   0.778 Preprocessor1_Model078\n\n\nNow using the testing dataset, we can see how well the final model fit the testing data. While not the best at predicting washed green coffee beans, this was a good test to show that the penalized regressions are not always the best fitting models compared to regular logistic regression. In the end, it seemed like the recipe was the most important component to predicting washed green coffee beans.\n\n\n! train/test split: preprocessor 1/1: Non-positive values in selected variable., No Box-Cox ...\n\n\n\nfinal_results %>%\n  collect_metrics()\n\n# A tibble: 2 x 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.823 Preprocessor1_Model1\n2 roc_auc  binary         0.817 Preprocessor1_Model1"
  },
  {
    "objectID": "posts/2022-06-02-prophet-model/index.html",
    "href": "posts/2022-06-02-prophet-model/index.html",
    "title": "Prophet Model",
    "section": "",
    "text": "As I start looking for non-academic positions, I wanted to practice forecasting as I didn’t really have much experience with these types of models. NOTE: This is for practicing forecasting skills and you should not trust this model with your own stocks. After plenty of reading,\nI finally have some understanding of how to utilize these models. This post started because even after a BA, 2 masters degrees, and a doctorate, my brother still has no clue what I do. He, along with most of my family think I am a Clinical Psychologist.\nSo for me to try and make my brother understand what I do, I thought I would show him with something that he has become interested with recently; stocks. So for this post, I’ll\nBelow are all the sites for the packages I used."
  },
  {
    "objectID": "posts/2022-06-02-prophet-model/index.html#loading-data",
    "href": "posts/2022-06-02-prophet-model/index.html#loading-data",
    "title": "Prophet Model",
    "section": "Loading Data",
    "text": "Loading Data\nTo load the Google Finance data, I decided to pick a stock that my brother had, which in this case was JetBlue. A cool feature about Google Finance and Google Sheets is that you can use the following formula in a Google Sheet on the first cell of the first column =GOOGLEFINANCE(\"JBLU\", \"price\", DATE(2000,1,1), DATE(2025, 1, 1), \"DAILY\") and it will give you the date and stock closing values for whatever period you’d like. The example above provides Google financial data for JBLU or the abbreviation for JetBlue stock. It also provides the price of the stock from the first day that there is data on JetBlue stocks, which in this case is April 12th 2002. You can also choose the period of time for the stock prices. I decided to look at daily data.\nJetBlue Sheet\nHere I have a copy of my Google Sheet for JetBlue that I will use to train and test my Prophet model. Instead of having a .csv file on my local machine, I decided to keep this on Google Drive so that it constantly updates with the Google Finance function. This meant that I had to use the googlesheets4 package to load the data from a Google Sheet. I also changed the name and class of the date variable to make it a date variable instead of a date and time variable.\n\ngooglesheets4::gs4_deauth()\n\ntheme_set(theme_light())\n\njet <- \n  googlesheets4::read_sheet('https://docs.google.com/spreadsheets/d/1SpRXsC3kXDaQLUfC6cPIOvsqxDF6updhgHRJeT8PTog/edit#gid=0') %>% \n  janitor::clean_names() %>% \n  mutate(ds = as_date(date))\n\n\nCleaning Up the Data\nBased on some visualizations below, I also decided to create some additional variables from the date variable. Specifically, I used lubridate's wday() function to create a new variable that gives you the actual day from the corresponding cell’s date. I also used the ts_clean_vec function from time_tk to clean for outliers in the stock price values. There are additional arguments for the function, like applying a Box-Cox transformation but that is for a multiplicative trend, which this model does not appear to fit since the variation in the outcome does not grow exponentially. I’ll also include 2002 as the reference year for the year variable and make sure that my data is arranged by date.\n\njetblue <- jet %>% \n  mutate(actual_day = wday(ds,\n                           label = TRUE),\n         clean = ts_clean_vec(close)) %>% \n  separate(col = date,\n           into = c('year_num', 'month_num', 'day_num'),\n           sep = '-') %>% \n  mutate(year_num = as.factor(year_num),\n         year_num = relevel(year_num, ref = '2002')) %>% \n  separate(col = day_num,\n           into = c('day_num', 'drop'),\n           sep = ' ') %>%\n  mutate(day_num = as.numeric(day_num),\n         month_num = as.factor(month_num)) %>% \n  select(-drop) %>% \n  arrange(ds)"
  },
  {
    "objectID": "posts/2022-06-02-prophet-model/index.html#visualizing-data",
    "href": "posts/2022-06-02-prophet-model/index.html#visualizing-data",
    "title": "Prophet Model",
    "section": "Visualizing Data",
    "text": "Visualizing Data\nStarting with some quick visualizations, we can see that the only area that there is a difference in the variation of the stock prices is in the beginning of 2020. I wonder what that could have been .\n\njetblue %>% \n  group_by(year_num, month_num) %>% \n  summarize(var_value = sd(close)^2) %>% \n  ungroup() %>% \n  ggplot(aes(month_num, var_value)) + \n  geom_point() + \n  facet_wrap(vars(year_num))\n\n\n\n\nNext, we can look at the histograms for the outcome of interest. If we look at the histograms, we can see that there are potential outliers in the original stock prices data. We can also see that cleaning the variable removed the potential outliers.\n\nonly_numeric <- jetblue %>% \n  select(close, clean)\n\nmap2(only_numeric,\n     names(only_numeric),\n     ~ggplot(data = only_numeric,\n             aes(.x)) + \n       geom_histogram(color = 'white',\n                      fill = 'dodgerblue') +\n       geom_vline(xintercept = mean(.x) +\n                    sd(.x) +\n                    sd(.x) +\n                    sd(.x),\n                  color = 'red',\n                  size = 1.25,\n                  linetype = 2) + \n       geom_vline(xintercept = mean(.x) -\n                    sd(.x) -\n                    sd(.x) -\n                    sd(.x),\n                  color = 'red',\n                  size = 1.25,\n                  linetype = 2) + \n       labs(title = .y))\n\n$close\n\n\n\n\n\n\n$clean\n\n\n\n\n\nThere will also be a lot of use of the purrr package and the map functions, which are part of the tidyverse. We can also see that in the plot series visualization using modeltime's plot_time_series function, that the cleaned stock prices remove the outliers. So from here on out, I’ll be using the cleaned stock prices.\n\nmap2(only_numeric,\n     names(only_numeric),\n     ~only_numeric %>% \n       plot_time_series(jetblue$ds,\n                        .x,\n                        .interactive = FALSE) + \n       labs(title = .y))\n\n$close\n\n\n\n\n\n\n$clean\n\n\n\n\n\nWe can also look for anomalies, or points that deviate from the trend. Using the plot_anomaly_diagnostics function from the modeltime package, I can see all the anomalies in the data. I also used ggplot to create my own visualization using the same data. Lastly, we’ll deal with those anomalies by removing them from the dataset. This is not too much of a problem because the Prophet model should be able to handle this fairly easy.\n\njetblue %>% \n  plot_anomaly_diagnostics(ds,\n                           clean,\n                           .facet_ncol = 1,\n                           .interactive = FALSE)\n\n\n\njetblue %>% \n  tk_anomaly_diagnostics(ds,\n                         clean) %>% \n  ggplot(aes(ds, observed)) + \n  geom_line() + \n  geom_point(aes(color = anomaly)) +\n  viridis::scale_color_viridis(option = 'D',\n                               discrete = TRUE,\n                               begin = .5,\n                               end = 0)\n\n\n\nanomaly <- jetblue %>%\n  tk_anomaly_diagnostics(ds,\n                         clean)\n\njetblue <- left_join(jetblue, anomaly) %>%\n  filter(anomaly != 'Yes')\n\nWe can also look into additional regressors to include in the model by looking into seasonality. We can see some fluctuation in stock prices across the years. We’ll include the year variable as another regressor on the stock prices.\n\njetblue %>% \n  plot_seasonal_diagnostics(ds,\n                            clean,\n                            .interactive = FALSE)"
  },
  {
    "objectID": "posts/2022-06-02-prophet-model/index.html#training-the-prophet-model",
    "href": "posts/2022-06-02-prophet-model/index.html#training-the-prophet-model",
    "title": "Prophet Model",
    "section": "Training the Prophet Model",
    "text": "Training the Prophet Model\nBefore we begin, I’m going to designate 10 cores to process any models run.\n\nset.seed(05262022)\n\nparallel::detectCores()\n\n[1] 12\n\nparallel_start(10,\n               .method = 'parallel')\n\nFirst, instead of the normal initial_split used for training and testing splits, we’ll use the initial_time_split function from tidymodels to separate the first 80% of the data into training set and the other 20% into the testing set.\n\nset.seed(05262022)\njet_split <- initial_time_split(jetblue)\n\n\nProphet Model Function\nI decided to create my own Prophet function to be able to use for both training the model and testing it. In this function, I’ve also included parameters that can be changed to see if the model performs better or worse. Lastly, the train = TRUE allows us to practice with the training dataset and then when we’re happy with the model, we can use it to test our model. For our model, we’ll be predicting stock prices with date and comparing each year to the reference year (2002).\n\nprophet_mod <- function(splits,\n                        changepoints = .05,\n                        seasonality = .01,\n                        holiday = .01,\n                        season_type = 'additive',\n                        day_season = 'auto',\n                        week_season = 'auto',\n                        year_season = 'auto',\n                        train = TRUE){\n  library(tidyverse)\n  library(tidymodels)\n  library(modeltime)\n  library(prophet)\n  \n  analy_data <- analysis(splits)\n  assess_data <- assessment(splits)\n  \n  model <- prophet_reg() %>% \n    set_engine(engine = 'prophet',\n               verbose = TRUE) %>% \n    set_args(prior_scale_changepoints = changepoints,\n             prior_scale_seasonality = seasonality,\n             prior_scale_holidays = holiday,\n             season = season_type,\n             seasonality_daily = day_season,\n             seasonality_weekly = week_season,\n             seasonality_yearly = year_season) %>% \n    fit(clean ~ ds + year_num, \n        data = analy_data)\n  \n  if(train == TRUE){\n    train_cali <- model %>% \n      modeltime_calibrate(new_data = analy_data)\n    \n    train_acc <- train_cali %>% \n      modeltime_accuracy()\n    \n    return(list(train_cali, train_acc))\n  }\n  \n  else{\n    test_cali <- model %>% \n      modeltime_calibrate(new_data = assess_data)\n    \n    test_acc <- test_cali %>% \n      modeltime_accuracy()\n    \n    return(list(test_cali, test_acc))\n  }\n}\n\nIt is worth noting that I’m using the modeltime package to run the prophet model because I believe it is easier to use (especially for later steps) than from Prophet but both can be implemented in this function. Let’s try running this model with the some random parameters I chose from the Prophet website until realizing that the modeltime parameters are log transformed.\n\nset.seed(05262022)\nbaseline <- prophet_mod(jet_split,\n                 train = TRUE) %>% \n  pluck(2)\n\nDisabling daily seasonality. Run prophet with daily.seasonality=TRUE to override this.\n\n\nConverting to Modeltime Table.\n\nbaseline\n\n# A tibble: 1 x 9\n  .model_id .model_desc           .type    mae  mape  mase smape  rmse   rsq\n      <int> <chr>                 <chr>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1         1 PROPHET W/ REGRESSORS Fitted 0.958  9.08  4.49  8.89  1.32 0.950\n\n\nSo with the model, we can see that the Mean Absolute Scaled Error (MASE) is 4.4874977 and the Root Mean Square Error (RMSE) is 1.317473. Not bad for an initial run. Let’s look at how the model fits the training data.\n\nprophet_mod(jet_split,\n                 train = TRUE) %>%  \n  pluck(1) %>% \n  modeltime_forecast(new_data = training(jet_split),\n                     actual_data = jetblue) %>% \n  plot_modeltime_forecast(.interactive = FALSE) +\n  labs(title = 'Prophet Baseline Model')\n\n\n\n\nSo the model appears to follow the trend line. We’ll try to tune some of these parameters to see if we can make the model better.\n\n\nTuning the Model\nNow, I’ll tune the prior scale values for the model. I’ll use the grid_latin_hypercube from the dials package in tidymodels to choose 5 sets of parameter values to run. I’m also using the rolling_origin from the rsample package in tidymodels because we are working with time series data. This does not create random samples but instead has samples with data points with consecutive values.\n\nset.seed(05262022)\n\nproph_model <- prophet_reg() %>%\n  set_engine(engine = 'prophet',\n             verbose = TRUE) %>%\n  set_args(prior_scale_changepoints = tune(),\n           prior_scale_seasonality = tune(),\n           prior_scale_holidays = tune(),\n           season = 'additive',\n           seasonality_daily = 'auto',\n           seasonality_weekly = 'auto',\n           seasonality_yearly = 'auto')\n\nproph_rec <-\n  recipe(clean ~ ds + year_num,\n         data = training(jet_split))\n\n\nset.seed(05262022)\ntrain_fold <-\n  rolling_origin(training(jet_split),\n                 initial = 270,  \n                 assess = 90, \n                 skip = 30,\n                 cumulative = TRUE)\n\nset.seed(05262022)\ngrid_values <-\n  grid_latin_hypercube(prior_scale_changepoints(),\n                       prior_scale_seasonality(),\n                       prior_scale_holidays(),\n                       size = 5)\n\nset.seed(05262022)\nproph_fit <- tune_grid(object = proph_model,\n                       preprocessor = proph_rec,\n                       resamples = train_fold,\n                       grid = grid_values,\n                       control = control_grid(verbose = TRUE,\n                                              save_pred = TRUE,\n                                              allow_par = TRUE))\n\n\ntuned_metrics <- collect_metrics(proph_fit)\ntuned_metrics %>%\n  filter(.metric == 'rmse') %>% \n  arrange(mean)\n\nsaveRDS(tuned_metrics,\n        file = 'tuned_metrics.rds')\n\n\nmetrics <-\n  readr::read_rds('C:/Users/cpppe/Desktop/github_projects/JP_posts/posts/2022-06-02-prophet-model/tuned_metrics.rds')\n\nmetrics %>% \n  filter(.metric == 'rmse') %>% \n  arrange(mean)\n\n# A tibble: 5 x 9\n  prior_scale_chan~1 prior~2 prior~3 .metric .esti~4  mean     n std_err .config\n               <dbl>   <dbl>   <dbl> <chr>   <chr>   <dbl> <int>   <dbl> <chr>  \n1            3.53    1.70e-2 1.12e+0 rmse    standa~  2.43   110   0.193 Prepro~\n2            0.884   3.64e+1 1.31e-2 rmse    standa~  2.56   110   0.195 Prepro~\n3            0.00139 1.66e-3 1.72e-3 rmse    standa~  2.56   110   0.211 Prepro~\n4            0.0549  2.61e-1 2.31e-1 rmse    standa~  2.66   110   0.204 Prepro~\n5           43.0     3.80e+0 1.22e+1 rmse    standa~  2.93   110   0.238 Prepro~\n# ... with abbreviated variable names 1: prior_scale_changepoints,\n#   2: prior_scale_seasonality, 3: prior_scale_holidays, 4: .estimator\n\n\nFor the sake of not waiting for this to render, I decided to make a RDS file of the metrics gathered from the tuned Prophet model. We can see that the RMSE value was 2.4252669 and the prior scale changepoint value was 3.5347457, the prior scale seasonality value was 0.0170306, and the prior scale holiday value was 1.1198542.\n\n\nFinal Training Model\nI then decided to run the prophet model on the training dataset with the new parameter values.\n\nfinal_train <- prophet_mod(jet_split,\n                 changepoints = 3.53,\n                 seasonality = .017,\n                 holiday = 1.12,\n                 train = TRUE) %>%  \n  pluck(2)\n\nfinal_train\n\n# A tibble: 1 x 9\n  .model_id .model_desc           .type    mae  mape  mase smape  rmse   rsq\n      <int> <chr>                 <chr>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1         1 PROPHET W/ REGRESSORS Fitted 0.841  7.84  3.94  7.73  1.21 0.957\n\nprophet_mod(jet_split,\n            changepoints = 3.53,\n            seasonality = .017,\n            holiday = 1.12,\n            train = TRUE) %>%  \n  pluck(1) %>% \n  modeltime_forecast(new_data = training(jet_split),\n                     actual_data = jetblue) %>% \n  plot_modeltime_forecast(.interactive = FALSE) +\n  labs(title = 'JetBlue Stock Prices - Training Model')\n\n\n\n\nWe can see that when using the whole training set, we have a RMSE of 1.2140588 and a MASE of 3.9392941 so both metrics reduced slightly."
  },
  {
    "objectID": "posts/2022-06-02-prophet-model/index.html#testing-the-model",
    "href": "posts/2022-06-02-prophet-model/index.html#testing-the-model",
    "title": "Prophet Model",
    "section": "Testing the Model",
    "text": "Testing the Model\nFinally, let’s test our Prophet model to see how well the model fits.\n\nprophet_mod(jet_split,\n            changepoints = 3.53,\n            seasonality = .017,\n            holiday = 1.12,\n            train = FALSE) %>%\n  pluck(1) %>% \n  modeltime_forecast(new_data = testing(jet_split),\n                     actual_data = jetblue) %>% \n  plot_modeltime_forecast(.interactive = FALSE) +\n  labs(title = 'JetBlue Stock Prices - Testing Model')\n\n\n\ntest_model <- prophet_mod(jet_split,\n            changepoints = 3.53,\n            seasonality = .017,\n            holiday = 1.12,\n            train = FALSE) %>%\n  pluck(2)\n\ntest_model\n\n# A tibble: 1 x 9\n  .model_id .model_desc           .type   mae  mape  mase smape  rmse   rsq\n      <int> <chr>                 <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1         1 PROPHET W/ REGRESSORS Test   18.2  131.  65.0  68.9  20.8 0.481\n\n\nWell, that doesn’t look very good and we can see that with the metrics. The MASE has gotten much worse (65.0040752) and so has the RMSE (20.7536337)"
  },
  {
    "objectID": "posts/2022-06-02-prophet-model/index.html#forecasting-ahead-a-year",
    "href": "posts/2022-06-02-prophet-model/index.html#forecasting-ahead-a-year",
    "title": "Prophet Model",
    "section": "Forecasting Ahead a Year",
    "text": "Forecasting Ahead a Year\nWell our model did not fit well to the testing data, but let’s see how it model looks when refit to the full data and forecasted forward a year. So in a year, it seems that JetBlue stock will remain roughly around the same value. It is important to note that the confidence intervals are large and with 95% confidence that values could be between 52.49 and -28.39 (not possible), there is not much confidence that JetBlue stock prices will remain where they are now in a year.\n\nfuture <- jetblue %>% \n  future_frame(.length_out = '1 year', .bind_data = TRUE)\n\nfuture <-\n  future %>%\n  select(-year_num, -month_num, -day_num) %>%\n  mutate(date2 = ds) %>%\n  separate(col = date2,\n           into = c('year_num', 'month_num', 'day_num'),\n           sep = '-') %>%\n  mutate(year_num = as.factor(year_num),\n         year_num = relevel(year_num, ref = '2002'),\n         month_num = as.factor(month_num),\n         day_num = as.numeric(day_num)) %>% \n  arrange(ds)\n\nglimpse(future)\n\nRows: 5,408\nColumns: 17\n$ close         <dbl> 13.33, 13.40, 13.57, 13.36, 13.10, 12.93, 12.45, 12.56, ~\n$ ds            <date> 2002-04-12, 2002-04-15, 2002-04-16, 2002-04-17, 2002-04~\n$ actual_day    <ord> Fri, Mon, Tue, Wed, Thu, Fri, Mon, Tue, Wed, Thu, Fri, M~\n$ clean         <dbl> 13.33, 13.40, 13.57, 13.36, 13.10, 12.93, 12.45, 12.56, ~\n$ observed      <dbl> 13.33, 13.40, 13.57, 13.36, 13.10, 12.93, 12.45, 12.56, ~\n$ season        <dbl> -0.004656862, 0.002322156, -0.005641252, 0.000500758, 0.~\n$ trend         <dbl> 13.40744, 13.41698, 13.42651, 13.43605, 13.44559, 13.455~\n$ remainder     <dbl> -0.072783284, -0.019298932, 0.149127845, -0.076550796, -~\n$ seasadj       <dbl> 13.33466, 13.39768, 13.57564, 13.35950, 13.09252, 12.934~\n$ remainder_l1  <dbl> -2.230554, -2.230554, -2.230554, -2.230554, -2.230554, -~\n$ remainder_l2  <dbl> 2.251864, 2.251864, 2.251864, 2.251864, 2.251864, 2.2518~\n$ anomaly       <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"N~\n$ recomposed_l1 <dbl> 11.17223, 11.18875, 11.19032, 11.20600, 11.22251, 11.219~\n$ recomposed_l2 <dbl> 15.65465, 15.67116, 15.67274, 15.68841, 15.70493, 15.702~\n$ year_num      <fct> 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 20~\n$ month_num     <fct> 04, 04, 04, 04, 04, 04, 04, 04, 04, 04, 04, 04, 04, 05, ~\n$ day_num       <dbl> 12, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 29, 30, 1, 2~\n\ntest_model1 <- prophet_mod(jet_split,\n            changepoints = 3.53,\n            seasonality = .017,\n            holiday = 1.12,\n            train = FALSE) %>%\n  pluck(1)\n\ntest_model1 %>% \n  modeltime_refit(data = future) %>% \n  modeltime_forecast(new_data = future,\n                     actual_data = jetblue) %>% \n  plot_modeltime_forecast(.interactive = FALSE) +\n  labs(title = 'Forecasted JetBlue Stock Prices')"
  }
]